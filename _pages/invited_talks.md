---
layout: page
title: Invited Speakers
permalink: /talks/
---

We are pleased to announce the following invited talks:

<!---  - **Sunday** ---><!--- **9:00 AM EST: ** --->
  - <span id= "Cohn_talk"></span> <b> Herbert A. Simon Prize Talk:<br>
  [Anthony Cohn]({{site.baseurl }}/speakers/Anthony_Cohn) --  [Professor of Automated Reasoning, University of Leeds](https://eps.leeds.ac.uk/computing/staff/76/professor-anthony-g-cohn-freng-ceng-citp) <br>
    **Title:** </b>TBD<br>
    **Abstract:** 


<!--- - **Saturday** ---> <!--- ** 11:00 AM EST: ** --->

 - <span id  ="Si_talk"></span><b>[Mei Si](https://faculty.rpi.edu/mei-si) -- Associate Professor, Cognitive Science Department, Rensselaer Polytechnic Institute<br>
  **Title:** </b>TBD<br>
  **Abstract:** 
    <br><br>
    

  
  - <span id="Byrne_talk"></span> <b><!--- **11:00 AM EST:** ---> 
   [Ruth Byrne](https://www.tcd.ie/research/profiles/?profile=rmbyrne),
  Professor of Cognitive Science, Psychology, Trinity Inst. of
  Neurosciences (TCIN) 
    
    **Title:** <b>How people reason about counterfactual explanations </b> <br>
    
    **Abstract:** People often create explanations about how an outcome could have turned out differently, if some preceding events had been different. In this talk I focus on the cognitive processes that underlie the construction and comprehension of counterfactual explanations and causal explanations. I discuss recent experimental discoveries of differences in how people reason about counterfactual and causal assertions, including evidence from eye-tracking experiments of differences in the tendency to consider multiple possibilities,  and evidence from sentence-stem completion experiments of differences in the content of such explanations. I consider some of the implications of these discoveries for the use of automated counterfactual explanations for decisions by Artificial Intelligence (AI) decision support systems in eXplainable AI (XAI). I describe empirical findings of differences in people’s subjective preferences for counterfactual and causal explanations for decisions by AI systems in familiar and unfamiliar domains, and people’s objective accuracy in predicting such decisions or making their own decisions. I argue that counterfactual explanations confer several cognitive benefits despite their potential cognitive costs.     
    <br><br>
    
<!---- **Monday:**<br>        --->    
  - <span id="Lenat_talk"> </span> <b>
   <!--- **11:00 AM EST**: -->
   [Doug Lenat](https://cyc.com/leadership-team/), CEO,
   [CYCORP](https://cycorp.com) <br>
   
    **Title:** <b>Computers versus Common Sense </b>  <br>
    
    **Abstract:**     [(See also the Cyc website)]( https://en.wikipedia.org/wiki/Cyc)<br>
  Almost everyone who talks about Artificial Intelligence, nowadays, means training deep neural nets on big data.  Developing and using those patterns is a lot like the cognition historically attributed to our right brain hemispheres; it enables AI's to react quickly and -- very often -- adequately.  But we human beings also make good use of the sort of cognition historically attributed to our left brain hemisphere, reasoning slowly, logically, and causally.   I will discuss this "other type of AI"-- i.e., symbolic AI, which comprises a formal representation language, a "seed" knowledge base with hand-engineered default rules of common sense and domain knowledge written in that language, and an inference engine capable of producing hundreds-deep chains of deduction, induction, and abduction on that large knowledge base.  I will describe the largest such platform, Cyc, a few commercial applications that were produced just by educating it as one might teach a new human employee, and give a short demo.   We've made a lot of mistakes, and learned a lot of lessons, in the last four decades, in trying to get such an AI to operate on without having to compromise on speed or on the expressiveness of its representation.  But it is important to remember that human beings' "super-power" is our ability to harness both types of reasoning, and I believe that the most powerful AI solutions in the coming decade will likewise be hybrids of the two.  That is the only path I see by which we will overcome the current dangerous inability of deep-learning AI's to understand and explain their decisions, and will make AI's far more trusted and -- more importantly -- far more trustworthy.
    <br><br>
    
<!--
**1:45 PM EST: <span id="panel"> Panel Discussion: Research Directions for Cognitive Systems</span>**
<table style="border-style: none; top-margin:-10px; border-spacing: 0px">
<tr style="border-style: none"> <td width=30px style="border-style: none"></td>
<td style="border-style: none">
<b>Abstract:</b> Given recent rapid advances in many areas of Artificial Intelligence, including but not limited to machine learning, access to large data, knowledge-graphs, robotics and autonomous vehicles to name a few, this community has a great opportunity to grow by demonstrating the relevance of its approaches through cross-fertilization with other research that could benefit from a cognitive systems architectural perspective and vice versa.  This panel will lead a group discussion on how we might do that, and thereby encourage more diverse participation at this conference.<br>
<b>Panelists:</b><br>
<ul>
<li><a href="https://eps.leeds.ac.uk/computing/staff/76/professor-anthony-g-cohn-freng-ceng-citp">Anthony Cohn, University of Leeds</a></li>
<li><a href="http://www.matthewklenk.com/">Matthew Klenk, Toyota Research Institute</a></li>
<li><a href="https://laird.engin.umich.edu/">John Laird, University of Michigan</a></li>
<li>Moderator: <a href="https://www.smith.edu/academics/faculty/jamie-macbeth">Jaime Macbeth, Smith College</a></li>
</ul></td></tr></table>
-->


<!-- - _Cognitive Systems Pedagogy_: An overview of the workshop moderated by Tom Williams. -->


